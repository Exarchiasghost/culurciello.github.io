---
layout: post
title:  "Navigating the unsupervised learning landscape"
date:   2016-06-10 07:02:01
categories: tech
---

Unsupervised learning is the Holy Grail of Deep Learning. The goal of unsupervised learning is to create general systems that can be trained with little data. Very little data. 

Today Deep Learning models are trained on large supervised datasets. Meaning that for each data, there is a corresponding label. In the case of the popular ImageNet dataset, there are 1M images labeled by humans. 1000 images for each of the 1000 classes. It can take some effort to create such dataset, many months of work. Imagine now creating a dataset with 1M classes. Imagine having to label each frame of a video dataset, with 100M frames. This is not scalable.

Now, think about how you got trained when you were very little. Yes you got some supervision, but when your parents told you that is a "cat" they would not tell you "cat" every split second you were looking at a cat for the rest of your life! That is what supervised learning is today: I tell you over and over what a "cat" is, maybe 1M times. Then your Deep Learning model gets it.

Ideally, we would like to have a model that behaves more like our brain. That needs just a few labels here and there to make sense of the multitude of classes of the world. And with classes I mean objects classes, action classes, environment classes, object parts classes, and the list goes on and on.

# general concepts

The main goal of unsupervised learning research is to pre-train a model  (called "discriminator" or "encoder") network to be used for other tasks. The encoder features should be general enough to be used in a categorization tasks: for example to train on ImageNet and provide good results, as close as possible as supervised models.

Up to date, supervised models always perform better than unsupervised pre-trained models. That is because the supervision allows to model to better encode the characteristics of the dataset. But supervision can also be decremental if the model is then applied to other tasks. In this regards, the hope is that unsupervised training can provide more general features for learning to perform any tasks.


# auto-encoders

Originated from [Bruno Olshausen and David Field](http://redwood.psych.cornell.edu/papers/olshausen_field_1997.pdf) in 1996. This paper showed that coding theory can be applied to the receptive field in the visual cortex. They showed that the primary visual vortex (V1) in our brain uses principles of sparsity to create a minimal set of base functions that can be also used to reconstruct the input image.

A great review is [here](https://piotrmirowski.files.wordpress.com/2014/03/piotrmirowski_2014_reviewautoencoders.pdf)

Yann LeCun group also worked a lot in [this area](http://www.cs.nyu.edu/~yann/research/deep/). In this page you can see a great animation of how sparse filters V1-like are learned.

Stacked-auto encoders are also used, by repeating the process of training greedily layer by layer.

Auto-encoders methods are also called "direct-mapping" methods.



# stacked unsupervised layers


One technique uses k-means clustering techniques to learn filters at multiple layers. 

Our group named this technique: [Clustering Learning](http://arxiv.org/abs/1301.2820), [Clustering Connections](http://arxiv.org/abs/1306.0152) and [Convolutional Clustering](http://arxiv.org/abs/1511.06241), which very recently achieved very good results on the popular STL-10 unsupervised dataset.

Our work in this area was developed independently to the work of [Adam Coates and Andrew Ng](http://www-cs.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf).

Restricted Boltzmann machines (RBMs), deep Boltzmann
machines (DBMs), [Deep belief networks (DBNs)](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf) have been notoriously hard to train because of the numerical difficulties of solving their partition function. As such they have not been used widely to solve problems.


# generative models

Generative models try to create a categorization (discriminator or encoder) network and a model that generates images (generative model) at the same time. This method originated from the pioneering work of [Ian Goodfellow and Yoshua Bengio](http://arxiv.org/abs/1406.2661).

The generative adversarial model by Alec Radford, Luke Metz, Soumith Chintala named [DCGAN](https://arxiv.org/abs/1511.06434) instantiates one such model that got really awesome results. 

A good explanation of this model is [here](https://ishmaelbelghazi.github.io/ALI/) where this digram comes from:

![](/assets/unsup/gan_simple.svg)

DCGAN wdiscriminator is designed to tell if an input image is real, or coming from the dataset, or fake, coming from the generator. The generator takes a random noise vector (say 512 values) at the input and generates an image.

In the DCGAN, this generator network is:

![](/assets/unsup/dcgan.jpg)

while the discriminator is a standard neural network. See code below for details.

The key is to train both networks in parallel while not completely copying the dataset. You want these to generalize to unseen examples, so learning the dataset would not be of use.

Training code of DCGAN in Torch7 is also [provided](https://github.com/soumith/dcgan.torch), which allow for [great experimentation](https://www.facebook.com/yann.lecun/posts/10153269667222143). 

After both generator and discriminator network are trained, one can use both. The main goal was to train a nice discriminator network to be used for other tasks, for example categorization on other datasets. The generator can be used to generate images out of random vectors. These images have very interesting properties. First of all, they offer smooth transitions from the input space. See an example here, where they show the images produced by moving between 9 random input vectors: 

![](/assets/unsup/dcgan_int.jpg)

Also the input vector space offers mathematical properties, showing the learned features are organized by similarity:

![](/assets/unsup/dcgan_math.jpg)

The smooth space learned by the generator suggests that the discriminator also has similar properties, making it a great general feature extractor for encoding images. This should help the typical problem of CNN trained in discontinuous image datasets, where [adversarial noise makes them fail](https://arxiv.org/abs/1312.6199).



# learn from data models

[Unsupervised learning of visual representations by solving jigsaw puzzles](http://arxiv.org/abs/1603.09246) is a clever trick. The author break the image into a puzzle and train a deep neural network to solve the puzzle. The resulting network has one of the highest performance of pre-trained networks.

[Unsupervised learning of visual representations from image patches and locality](https://arxiv.org/abs/1511.06811) is also a clever trick. Here they take two patches of the same image closely located. These patches are statistically of the same object. A third patch is taken from a random picture and location, statistically not of the same object as the other 2 patches. Then a deep neural network is trained to discriminate between 2 patches of same object or different objects. The resulting network has one of the highest performance of pre-trained networks.

[Unsupervised Learning of Visual Representations using Videos](http://arxiv.org/abs/1505.00687)




# the future

